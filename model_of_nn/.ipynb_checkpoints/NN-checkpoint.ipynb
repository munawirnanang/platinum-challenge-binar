{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\666Rakels\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# lib standart\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split # Untuk split data\n",
    "from sklearn.neural_network import MLPClassifier # Untuk Algoritma ML yang akan di pakai\n",
    "from sklearn.model_selection import GridSearchCV  # untuk tuning hyperparameter\n",
    "from sklearn.pipeline import Pipeline # untuk membangun pipeline ML\n",
    "from sklearn.compose import ColumnTransformer # bagian dari pipe line untuk handling kolom \n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report # menghitung nilai f1\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle # menyimpan model\n",
    "import text_preprocessing as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>warung ini dimiliki oleh pengusaha pabrik tahu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mohon ulama lurus dan k212 mmbri hujjah partai...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lokasi strategis di jalan sumatera bandung . t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>betapa bahagia nya diri ini saat unboxing pake...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duh . jadi mahasiswa jangan sombong dong . kas...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10995</th>\n",
       "      <td>tidak kecewa</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10996</th>\n",
       "      <td>enak rasa masakan nya apalagi kepiting yang me...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10997</th>\n",
       "      <td>hormati partai-partai yang telah berkoalisi</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10998</th>\n",
       "      <td>pagi pagi di tol pasteur sudah macet parah , b...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999</th>\n",
       "      <td>meskipun sering belanja ke yogya di riau junct...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0         1\n",
       "0      warung ini dimiliki oleh pengusaha pabrik tahu...  positive\n",
       "1      mohon ulama lurus dan k212 mmbri hujjah partai...   neutral\n",
       "2      lokasi strategis di jalan sumatera bandung . t...  positive\n",
       "3      betapa bahagia nya diri ini saat unboxing pake...  positive\n",
       "4      duh . jadi mahasiswa jangan sombong dong . kas...  negative\n",
       "...                                                  ...       ...\n",
       "10995                                       tidak kecewa  positive\n",
       "10996  enak rasa masakan nya apalagi kepiting yang me...  positive\n",
       "10997        hormati partai-partai yang telah berkoalisi   neutral\n",
       "10998  pagi pagi di tol pasteur sudah macet parah , b...  negative\n",
       "10999  meskipun sering belanja ke yogya di riau junct...  positive\n",
       "\n",
       "[11000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv('train_preprocess.tsv', sep='\\t',header = None)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentimen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>warung ini dimiliki oleh pengusaha pabrik tahu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mohon ulama lurus dan k212 mmbri hujjah partai...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lokasi strategis di jalan sumatera bandung . t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>betapa bahagia nya diri ini saat unboxing pake...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duh . jadi mahasiswa jangan sombong dong . kas...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentimen\n",
       "0  warung ini dimiliki oleh pengusaha pabrik tahu...  positive\n",
       "1  mohon ulama lurus dan k212 mmbri hujjah partai...   neutral\n",
       "2  lokasi strategis di jalan sumatera bandung . t...  positive\n",
       "3  betapa bahagia nya diri ini saat unboxing pake...  positive\n",
       "4  duh . jadi mahasiswa jangan sombong dong . kas...  negative"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = data_train.rename(columns={0: 'Text', 1: 'Sentimen'})\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisasi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Text  Sentimen  \\\n",
      "0      warung ini dimiliki oleh pengusaha pabrik tahu...  positive   \n",
      "1      mohon ulama lurus dan k212 mmbri hujjah partai...   neutral   \n",
      "2      lokasi strategis di jalan sumatera bandung . t...  positive   \n",
      "3      betapa bahagia nya diri ini saat unboxing pake...  positive   \n",
      "4      duh . jadi mahasiswa jangan sombong dong . kas...  negative   \n",
      "...                                                  ...       ...   \n",
      "10995                                       tidak kecewa  positive   \n",
      "10996  enak rasa masakan nya apalagi kepiting yang me...  positive   \n",
      "10997        hormati partai-partai yang telah berkoalisi   neutral   \n",
      "10998  pagi pagi di tol pasteur sudah macet parah , b...  negative   \n",
      "10999  meskipun sering belanja ke yogya di riau junct...  positive   \n",
      "\n",
      "                                               Text_baru  \n",
      "0      warung milik usaha pabrik puluh kenal putih ba...  \n",
      "1      mohon ulama lurus k212 mmbri hujjah partai diw...  \n",
      "2      lokasi strategis jalan sumatera bandung man ut...  \n",
      "3      betapa positive unboxing paket barang positive...  \n",
      "4      duh mahasiswa negative ka kartu kuning ajar po...  \n",
      "...                                                  ...  \n",
      "10995                                           negative  \n",
      "10996  enak masakan kepiting positive pilih kepiting ...  \n",
      "10997                       hormat partai partai koalisi  \n",
      "10998  pagi pagi tol pasteur negative parah bikin neg...  \n",
      "10999  belanja yogya riau junction kali lihat foodlif...  \n",
      "\n",
      "[11000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import text_preprocessing as tp\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#load train dataset\n",
    "df_train = pd.read_csv('train_preprocess.tsv', sep='\\t',header = None)\n",
    "#df_train = pd.read_csv('/content/train_preprocess.tsv', sep='\\t', header=None)\n",
    "df_data = df_train.rename(columns={0: 'Text',1: 'Sentimen'})\n",
    "\n",
    "#load kamus alay dataset\n",
    "kamus_alay = pd.read_csv('new_kamusalay.csv', encoding='latin-1', header=None)\n",
    "#kamus_alay = pd.read_csv('/content/new_kamusalay.csv', encoding='latin-1', header=None)\n",
    "kamus_alay = kamus_alay.rename(columns={0: 'original', 1: 'replacement'})\n",
    "\n",
    "id_stopword_dict = pd.read_csv('stopwordbahasa.csv', header=None)\n",
    "#id_stopword_dict = pd.read_csv('/content/stopwordbahasa.csv', header=None)\n",
    "id_stopword_dict = id_stopword_dict.rename(columns={0: 'stopword'})\n",
    "\n",
    "\n",
    "# Define the preprocessing function\n",
    "#text cleansing with regex\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def hapus_karakter_ga_penting(text):\n",
    "    text = re.sub('\\n',' ',text) # Hapus enter\n",
    "    text = re.sub('nya|deh|sih',' ',text) # Hapus stopwords tambahan\n",
    "    text = re.sub('RT',' ',text) # Hapus RT\n",
    "    text = re.sub('USER',' ',text) # Hapus USER\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', ' ',text) # Hapus URL\n",
    "    text = re.sub('  +', ' ', text) # Hapus extra spaces\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', text) #Hapus non huruf dan angka  \n",
    "    text = re.sub('\\@[a-zA-Z0-9]*', ' ', text) #Hapus non huruf atau apostrophe\n",
    "    text = ' '.join([w for w in text.split() if len(w)>1]) #Hapus huruf tunggal \n",
    "    return text\n",
    "    \n",
    "def hapus_nonhurufangka(text):\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n",
    "    return text\n",
    "\n",
    "df_data_map = dict(zip(df_data['Text'], df_data['Sentimen']))\n",
    "def normalisasi_alay(text):\n",
    "    return ' '.join([df_data_map[word] if word in df_data_map else word for word in text.split(' ')])\n",
    "\n",
    "def hapus_stopword(text):\n",
    "    text = ' '.join(['' if word in id_stopword_dict.stopword.values else word for word in text.split(' ')])\n",
    "    text = re.sub('  +', ' ', text) # Hapus extra spaces\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def stemming(text):\n",
    "    return stemmer.stem(text)\n",
    "\n",
    "def cleansing(text):\n",
    "    text = lowercase(text) \n",
    "    text = hapus_nonhurufangka(text) \n",
    "    text = hapus_karakter_ga_penting(text) \n",
    "    text = normalisasi_alay(text) \n",
    "    text = stemming(text) \n",
    "    text = hapus_stopword(text) \n",
    "    return text\n",
    "\n",
    "\n",
    "# Add the new column to the DataFrame\n",
    "df_data['Text_baru'] = df_data['Text'].apply(cleansing)\n",
    "\n",
    "# Create the final DataFrame with the original columns\n",
    "data_train = df_data[['Text', 'Sentimen']].assign(Text_baru=df_data['Text_baru'])\n",
    "\n",
    "# Display the result\n",
    "print(df_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_baru</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>warung ini dimiliki oleh pengusaha pabrik tahu...</td>\n",
       "      <td>warung milik usaha pabrik puluh kenal putih ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mohon ulama lurus dan k212 mmbri hujjah partai...</td>\n",
       "      <td>mohon ulama lurus k212 mmbri hujjah partai diw...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lokasi strategis di jalan sumatera bandung . t...</td>\n",
       "      <td>lokasi strategis jalan sumatera bandung man ut...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>betapa bahagia nya diri ini saat unboxing pake...</td>\n",
       "      <td>betapa positive unboxing paket barang positive...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duh . jadi mahasiswa jangan sombong dong . kas...</td>\n",
       "      <td>duh mahasiswa negative ka kartu kuning ajar po...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  warung ini dimiliki oleh pengusaha pabrik tahu...   \n",
       "1  mohon ulama lurus dan k212 mmbri hujjah partai...   \n",
       "2  lokasi strategis di jalan sumatera bandung . t...   \n",
       "3  betapa bahagia nya diri ini saat unboxing pake...   \n",
       "4  duh . jadi mahasiswa jangan sombong dong . kas...   \n",
       "\n",
       "                                           Text_baru  negative  neutral  \\\n",
       "0  warung milik usaha pabrik puluh kenal putih ba...         0        0   \n",
       "1  mohon ulama lurus k212 mmbri hujjah partai diw...         0        1   \n",
       "2  lokasi strategis jalan sumatera bandung man ut...         0        0   \n",
       "3  betapa positive unboxing paket barang positive...         0        0   \n",
       "4  duh mahasiswa negative ka kartu kuning ajar po...         1        0   \n",
       "\n",
       "   positive  \n",
       "0         1  \n",
       "1         0  \n",
       "2         1  \n",
       "3         1  \n",
       "4         0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "category = pd.get_dummies(data_train.Sentimen)\n",
    "df_baru = pd.concat([data_train, category], axis=1)\n",
    "df_baru = df_baru.drop(columns='Sentimen')\n",
    "\n",
    "df_baru['Text'] = df_baru['Text'].replace('\\n', ' ').str.lower()\n",
    "\n",
    "df_baru.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "melakukan feature Extration menggunakan metode Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed = df_data.Text_baru.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extration selesai\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "    \n",
    "# melakukan fitting dan transformasi pada dokumen\n",
    "count_vect.fit(data_preprocessed)\n",
    "\n",
    "# melihat hasil representasi bag of words\n",
    "X = count_vect.fit_transform(data_preprocessed)\n",
    "\n",
    "print(\"Feature extration selesai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menyimpan objek count_vect \n",
    "\n",
    "pickle.dump(count_vect, open(\"feature_New3.sav\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_train.Sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df_data.Sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        positive\n",
       "1         neutral\n",
       "2        positive\n",
       "3        positive\n",
       "4        negative\n",
       "           ...   \n",
       "10995    positive\n",
       "10996    positive\n",
       "10997     neutral\n",
       "10998    negative\n",
       "10999    positive\n",
       "Name: Sentimen, Length: 11000, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, classes, test_size = 0.20, random_state = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = Pipeline([('algoritma', MLPClassifier())])\n",
    "parameter_grid = {\n",
    "    'algoritma__hidden_layer_sizes': [(i,) for i in [1, 10, 20]],\n",
    "    'algoritma__activation': ['relu', 'tanh', 'logistic'],\n",
    "    'algoritma__learning_rate_init': [0.01, 0.1],\n",
    "}\n",
    "\n",
    "model_NN3 = GridSearchCV(model, parameter_grid, cv=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_NN3 = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Binar\\Data Science\\Code\\envbinar\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing selesai\n",
      "CPU times: total: 19min 21s\n",
      "Wall time: 19min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_NN3.fit(X_train,y_train)\n",
    "\n",
    "print('testing selesai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algoritma__activation': 'logistic',\n",
       " 'algoritma__hidden_layer_sizes': (20,),\n",
       " 'algoritma__learning_rate_init': 0.1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_NN3.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_NN3, open(\"model_NN3.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluasi Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untuk Semua Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_NN3.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.91      0.93      3436\n",
      "     neutral       0.94      0.93      0.93      1148\n",
      "    positive       0.96      0.98      0.97      6416\n",
      "\n",
      "    accuracy                           0.95     11000\n",
      "   macro avg       0.95      0.94      0.95     11000\n",
      "weighted avg       0.95      0.95      0.95     11000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untuk Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model_NN3.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.96      0.98      2775\n",
      "     neutral       1.00      1.00      1.00       915\n",
      "    positive       0.98      1.00      0.99      5110\n",
      "\n",
      "    accuracy                           0.99      8800\n",
      "   macro avg       0.99      0.98      0.99      8800\n",
      "weighted avg       0.99      0.99      0.99      8800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untuk data testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model_NN3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.72      0.74       661\n",
      "     neutral       0.71      0.64      0.67       233\n",
      "    positive       0.88      0.92      0.90      1306\n",
      "\n",
      "    accuracy                           0.83      2200\n",
      "   macro avg       0.78      0.76      0.77      2200\n",
      "weighted avg       0.82      0.83      0.83      2200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ke- 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.70      0.72       680\n",
      "     neutral       0.73      0.64      0.68       239\n",
      "    positive       0.84      0.89      0.87      1281\n",
      "\n",
      "    accuracy                           0.80      2200\n",
      "   macro avg       0.77      0.74      0.76      2200\n",
      "weighted avg       0.80      0.80      0.80      2200\n",
      "\n",
      "======================================================\n",
      "Training ke- 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.72      0.71       706\n",
      "     neutral       0.66      0.61      0.63       220\n",
      "    positive       0.85      0.85      0.85      1274\n",
      "\n",
      "    accuracy                           0.78      2200\n",
      "   macro avg       0.74      0.73      0.73      2200\n",
      "weighted avg       0.78      0.78      0.78      2200\n",
      "\n",
      "======================================================\n",
      "Training ke- 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.74      0.74       682\n",
      "     neutral       0.73      0.65      0.68       215\n",
      "    positive       0.87      0.88      0.88      1303\n",
      "\n",
      "    accuracy                           0.82      2200\n",
      "   macro avg       0.78      0.76      0.77      2200\n",
      "weighted avg       0.82      0.82      0.82      2200\n",
      "\n",
      "======================================================\n",
      "Training ke- 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.74      0.73       698\n",
      "     neutral       0.70      0.56      0.62       229\n",
      "    positive       0.86      0.88      0.87      1273\n",
      "\n",
      "    accuracy                           0.80      2200\n",
      "   macro avg       0.76      0.72      0.74      2200\n",
      "weighted avg       0.80      0.80      0.80      2200\n",
      "\n",
      "======================================================\n",
      "Training ke- 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.79      0.74       670\n",
      "     neutral       0.72      0.58      0.64       245\n",
      "    positive       0.89      0.86      0.87      1285\n",
      "\n",
      "    accuracy                           0.81      2200\n",
      "   macro avg       0.77      0.74      0.75      2200\n",
      "weighted avg       0.81      0.81      0.81      2200\n",
      "\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "Rata-rata Accuracy:  0.8024545454545453\n"
     ]
    }
   ],
   "source": [
    "# Untuk lebih menyakinkan lagi, kita juga bisa melakukan \"Cross Validation\"\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5,random_state=42,shuffle=True)\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "y = classes\n",
    "\n",
    "for iteration, data in enumerate(kf.split(X), start=1):\n",
    "\n",
    "    data_train   = X[data[0]]\n",
    "    target_train = y[data[0]]\n",
    "\n",
    "    data_test    = X[data[1]]\n",
    "    target_test  = y[data[1]]\n",
    "\n",
    "    clf = MLPClassifier()\n",
    "    clf.fit(data_train,target_train)\n",
    "\n",
    "    preds = clf.predict(data_test)\n",
    "\n",
    "    # for the current fold only    \n",
    "    accuracy = accuracy_score(target_test,preds)\n",
    "\n",
    "    print(\"Training ke-\", iteration)\n",
    "    print(classification_report(target_test,preds))\n",
    "    print(\"======================================================\")\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# this is the average accuracy over all folds\n",
    "average_accuracy = np.mean(accuracies)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print(\"Rata-rata Accuracy: \", average_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text :  warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung . tahu berkualitas , dipadu keahlian memasak , dipadu kretivitas , jadilah warung yang menyajikan menu utama berbahan tahu , ditambah menu umum lain seperti ayam . semuanya selera indonesia . harga cukup terjangkau . jangan lewatkan tahu bletoka nya , tidak kalah dengan yang asli dari tegal !\n",
      "\n",
      "text_new :  warung milik usaha pabrik puluh kenal putih bandung positive padu ahli masak padu kretivitas warung me jikan menu utama bahan menu ayam selera indonesia harga jangkau bletoka kalah asli tegal\n",
      "\n",
      "Sentiment :  positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\666Rakels\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import text_preprocessing\n",
    "\n",
    "# Download the Indonesian stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "count_vect = pickle.load(open(\"feature_New3.sav\", \"rb\"))\n",
    "model_NN = pickle.load(open(\"model_NN3.sav\", \"rb\"))\n",
    "\n",
    "contoh = \"warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung . tahu berkualitas , dipadu keahlian memasak , dipadu kretivitas , jadilah warung yang menyajikan menu utama berbahan tahu , ditambah menu umum lain seperti ayam . semuanya selera indonesia . harga cukup terjangkau . jangan lewatkan tahu bletoka nya , tidak kalah dengan yang asli dari tegal !\"\n",
    "preprocessed_text = cleansing(contoh)\n",
    "text_vector = count_vect.transform([preprocessed_text])\n",
    "\n",
    "result = model_NN.predict(text_vector)[0]\n",
    "print(\"text : \", contoh)\n",
    "print(\"\\ntext_new : \", preprocessed_text)\n",
    "print(\"\\nSentiment : \", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envbinar",
   "language": "python",
   "name": "envbinar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
